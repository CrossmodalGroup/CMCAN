# CMCAN

Implementation of our AAAI2022 paper, Show Your Faith: Cross-Modal Confidence-Aware Network for Image-Text Matching.

## Data
vocab is available here: [vocab](https://drive.google.com/drive/folders/1Xxl2452lWrjWuXtFQgzItKRfhUdZRKFt?usp=drive_link).

ims_bbx.npy, ims_size.npy, precaps_stan.txt for Flickr30K: [f30k_precomp](https://drive.google.com/drive/folders/1kuLiCLb5luJdorJuKMVhT7BMVhJSybT-?usp=drive_link).

ims_bbx.npy, ims_size.npy, precaps_stan.txt for MSCOCO: [coco_precomp](https://drive.google.com/drive/folders/1iKUOq4NL14I3imhUuPB0sdF_DZHgB4o7?usp=drive_link).

ims_dir_selfadj{4, 8, 12}.npy can be created by running the adj.py under directory ./context_extractor/.

## Trained Model
The model trained on the Flickr30K dataset is available here: [checkpoint_f30k_c499.3.pth](https://drive.google.com/file/d/1x89dKj87PZh1Ke-hfxlU94ui6T7fdk31/view?usp=drive_link).

## Note
Any questions please contact huatianzhang@mail.ustc.edu.cn for immediate reply. Thanks.


